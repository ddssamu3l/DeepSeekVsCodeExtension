# LoCopilot - Open Source Locally-Running AI Copilot.

![alt text](LoCopilot_gif.gif)

## ‚ú® Features
- ### ü§ñ Local LLM Coding Assistant
* Latest open-source reasoning models from DeepSeek, Google, Qwen, Microsoft...
* Model switching without context loss
* Thought streaming
- ### Advanced Context Loading
* Opened files are automatically added to context
* LoCopilot can see user elected text 

  
## üöÄ Getting Started
Install Ollama from the download link: <a href="https://ollama.com/download">https://ollama.com/download</a>

Alternitively, install it manually
```
curl -fsSL https://ollama.com/install.sh | sh
```

## üí° Next Steps
After installing Ollama, be sure to install at least 1 model before running the extension.

Select from one of the available models (brackets indicate VRAM usage)
- DeepSeek R1 8b   (4.9GB)
- DeepSeek R1 70b  (43GB)
- Gemma 3 4b (3.3GB)
- Gemma 3 27b (17GB)
- QwQ (20GB)

### üìù Example (default model: DeepSeek R1 8b)
```
ollama pull deepseek-r1:8b
```

## ‚ù§Ô∏è Support
For any issues with the extension, reach out to us directly at samueldeng78@gmail.com
